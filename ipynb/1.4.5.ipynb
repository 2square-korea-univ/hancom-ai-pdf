{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성화 함수 및 손실 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 활성화 함수 및 미분\n",
    "def sigmoid(x): # 출력층\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x): # 은닉층\n",
    "    return np.maximum(0, x)\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# 손실 함수 (MSE) 및 미분\n",
    "def mse_loss(y_hat, y):\n",
    "    return 0.5 * np.mean((y_hat - y)2)\n",
    "def mse_loss_derivative(y_hat, y):\n",
    "    return (y_hat - y) / y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 클래스: 초기화 및 순전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # 가중치 및 편향 초기화\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 순전파 계산\n",
    "        self.z1 = np.dot(self.W1, x) + self.b1 # 은닉층 선형 변환\n",
    "        self.a1 = relu(self.z1) # 은닉층 활성화\n",
    "        self.z2 = np.dot(self.W2, self.a1) + self.b2 # 출력층 선형 변환\n",
    "        self.y_hat = sigmoid(self.z2) # 출력층 활성화 (예측값)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def backward(self, x, y, y_hat):\n",
    "        # 역전파 계산\n",
    "        m = y.size # 데이터 샘플 수\n",
    "\n",
    "        # 1. 출력층 오차 항수 (delta2) 계산\n",
    "        delta2 = mse_loss_derivative(y_hat, y) * sigmoid_derivative(self.z2)\n",
    "\n",
    "        # 2. 은닉층 오차 항수 (delta1) 계산 (역전파)\n",
    "        # delta1 = (W2.T * delta2) element-wise * relu_derivative(z1)\n",
    "        delta1 = np.dot(self.W2.T, delta2) * relu_derivative(self.z1)\n",
    "\n",
    "        # 3. 각 가중치 및 편향 기울기 계산\n",
    "        dW2 = np.dot(delta2, self.a1.T) / m\n",
    "        db2 = np.sum(delta2, axis=1, keepdims=True) / m\n",
    "        dW1 = np.dot(delta1, x.T) / m\n",
    "        db1 = np.sum(delta1, axis=1, keepdims=True) / m\n",
    "\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_params(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        # 가중치 및 편향 업데이트 (경사하강법)\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def train(self, X, Y, learning_rate, epochs):\n",
    "        # 지정된 epoch 수만큼 학습 반복\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self.forward(X) # 순전파\n",
    "            dW1, db1, dW2, db2 = self.backward(X, Y, y_hat) # 역전파\n",
    "            self.update_params(dW1, db1, dW2, db2, learning_rate) # 업데이트\n",
    "\n",
    "            loss = mse_loss(y_hat, Y) # 손실 계산\n",
    "            if epoch % 100 == 0: # 100 epoch마다 손실 출력\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 생성 (간단한 AND 게이트)\n",
    "# X: 2 features, 4 samples\n",
    "# Y: 1 output, 4 samples\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 0, 0, 1]])\n",
    "\n",
    "# 모델 생성 및 학습\n",
    "net = TwoLayerNet(input_size=2, hidden_size=3, output_size=1)\n",
    "net.train(X, Y, learning_rate=0.1, epochs=1000)\n",
    "\n",
    "# 학습 후 예측\n",
    "test_x1 = np.array([[1], [1]])\n",
    "prediction1 = net.forward(test_x1)\n",
    "print(f\"Prediction for [1, 1]: {prediction1[0][0]:.4f}\") # Expected: ~1\n",
    "\n",
    "test_x2 = np.array([[0], [1]])\n",
    "prediction2 = net.forward(test_x2)\n",
    "print(f\"Prediction for [0, 1]: {prediction2[0][0]:.4f}\") # Expected: ~0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras로 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# 1. 학습 데이터 생성 (AND 게이트)\n",
    "# X: 입력. 각 행이 하나의 샘플. (4 samples, 2 features)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "# Y: 출력. (4 samples, 1 output)\n",
    "Y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# 2. 모델 구성\n",
    "# 입력 특징 2개, 은닉층 뉴런 3개 (ReLU 활성화), 출력 뉴런 1개 (Sigmoid 활성화)\n",
    "model = Sequential([\n",
    "    Dense(3, activation='relu', input_shape=(2,)), # 은닉층\n",
    "    Dense(1, activation='sigmoid') # 출력층\n",
    "])\n",
    "\n",
    "# 3. 모델 컴파일\n",
    "# 옵티마이저: SGD (Stochastic Gradient Descent), 손실 함수: MSE (Mean Squared Error)\n",
    "# 학습률(learning_rate)은 SGD 옵티마이저 내에서 설정\n",
    "optimizer = SGD(learning_rate=0.1)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy']) # mse도 가능\n",
    "\n",
    "# 모델 요약 정보 출력\n",
    "model.summary()\n",
    "\n",
    "# 4. 모델 학습\n",
    "print(\"\\nTraining with Keras...\")\n",
    "# epochs를 1000으로 설정 (NumPy 예제와 유사하게)\n",
    "# verbose=0으로 설정하여 학습 중 로그 출력을 최소화할 수 있음\n",
    "# verbose=1이면 진행 막대 표시, verbose=2이면 에포크당 한 줄\n",
    "history = model.fit(X, Y, epochs=1000, verbose=0, batch_size=4) # 전체 데이터를 하나의 배치로 사용\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 학습 마지막 손실 값 출력\n",
    "final_loss = history.history['loss'][-1]\n",
    "print(f\"Final loss: {final_loss:.4f}\")\n",
    "\n",
    "\n",
    "# 5. 학습 후 예측\n",
    "print(\"\\nPredictions after training (Keras):\")\n",
    "predictions = model.predict(X)\n",
    "for i in range(X.shape[0]):\n",
    "    print(f\"Input: {X[i]}, Prediction: {predictions[i][0]:.4f} (Expected: {Y[i][0]})\")\n",
    "\n",
    "test_x1 = np.array([[1, 1]])\n",
    "prediction1 = model.predict(test_x1)\n",
    "print(f\"Prediction for [1, 1]: {prediction1[0][0]:.4f} (Expected: ~1)\")\n",
    "\n",
    "test_x2 = np.array([[0, 1]])\n",
    "prediction2 = model.predict(test_x2)\n",
    "print(f\"Prediction for [0, 1]: {prediction2[0][0]:.4f} (Expected: ~0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch로 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 0. 재현성을 위한 시드 설정 (선택 사항)\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "\n",
    "# 1. 학습 데이터 생성 (AND 게이트) 및 Tensor 변환\n",
    "# X: 입력. 각 행이 하나의 샘플. (4 samples, 2 features)\n",
    "X_numpy = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "# Y: 출력. (4 samples, 1 output)\n",
    "Y_numpy = np.array([[0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "X_torch = torch.tensor(X_numpy)\n",
    "Y_torch = torch.tensor(Y_numpy)\n",
    "\n",
    "# 2. 신경망 모델 정의\n",
    "# 입력 특징 2개, 은닉층 뉴런 3개 (ReLU 활성화), 출력 뉴런 1개 (Sigmoid 활성화)\n",
    "class TwoLayerNetPyTorch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TwoLayerNetPyTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # 입력층 -> 은닉층\n",
    "        self.relu = nn.ReLU()                         # ReLU 활성화\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # 은닉층 -> 출력층\n",
    "        self.sigmoid = nn.Sigmoid()                   # Sigmoid 활성화\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "model_pytorch = TwoLayerNetPyTorch(input_size, hidden_size, output_size)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(model_pytorch)\n",
    "\n",
    "# 3. 손실 함수 및 옵티마이저 정의\n",
    "# 손실 함수: MSE (Mean Squared Error)\n",
    "criterion = nn.MSELoss()\n",
    "# 옵티마이저: SGD (Stochastic Gradient Descent)\n",
    "learning_rate = 0.1\n",
    "optimizer_pytorch = optim.SGD(model_pytorch.parameters(), lr=learning_rate)\n",
    "\n",
    "# 4. 모델 학습\n",
    "print(\"\\nTraining with PyTorch...\")\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # 순전파\n",
    "    outputs = model_pytorch(X_torch)\n",
    "    loss = criterion(outputs, Y_torch)\n",
    "\n",
    "    # 역전파 및 가중치 업데이트\n",
    "    optimizer_pytorch.zero_grad() # 이전 에포크의 기울기 초기화\n",
    "    loss.backward()             # 역전파 수행 (기울기 계산)\n",
    "    optimizer_pytorch.step()      # 가중치 업데이트\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "print(\"Training finished.\")\n",
    "final_loss_pytorch = loss.item()\n",
    "print(f\"Final loss: {final_loss_pytorch:.4f}\")\n",
    "\n",
    "# 5. 학습 후 예측\n",
    "print(\"\\nPredictions after training (PyTorch):\")\n",
    "model_pytorch.eval() # 평가 모드로 전환 (Dropout, BatchNorm 등에 영향)\n",
    "with torch.no_grad(): # 기울기 계산 비활성화\n",
    "    predictions_pytorch = model_pytorch(X_torch)\n",
    "    for i in range(X_torch.shape[0]):\n",
    "        print(f\"Input: {X_numpy[i]}, Prediction: {predictions_pytorch[i].item():.4f} (Expected: {Y_numpy[i][0]})\")\n",
    "\n",
    "    test_x1_torch = torch.tensor([[1, 1]], dtype=torch.float32)\n",
    "    prediction1_torch = model_pytorch(test_x1_torch)\n",
    "    print(f\"Prediction for [1, 1]: {prediction1_torch.item():.4f} (Expected: ~1)\")\n",
    "\n",
    "    test_x2_torch = torch.tensor([[0, 1]], dtype=torch.float32)\n",
    "    prediction2_torch = model_pytorch(test_x2_torch)\n",
    "    print(f\"Prediction for [0, 1]: {prediction2_torch.item():.4f} (Expected: ~0)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler-arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

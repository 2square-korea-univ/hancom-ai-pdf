{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 24\uc7a5 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc9c1\uc811 \ub9cc\ub4e4\uc5b4 \ubcf4\uae30",
        "",
        "## \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub780?",
        "",
        "\ud2b8\ub79c\uc2a4\ud3ec\uba38\ub294 2017\ub144 \"Attention Is All You Need\" \ub17c\ubb38\uc5d0\uc11c \uc18c\uac1c\ub41c \ud601\uc2e0\uc801\uc778 \ub525\ub7ec\ub2dd \uc544\ud0a4\ud14d\ucc98\uc785\ub2c8\ub2e4. RNN\uc774\ub098 CNN \uc5c6\uc774 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\ub9cc\uc73c\ub85c \uc2dc\ud000\uc2a4 \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub294 \ubc29\uc2dd\uc744 \uc81c\uc548\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc740 BERT, GPT \ub4f1 \ud604\ub300 \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uae30\ubc18\uc774 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.",
        "",
        "### \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c",
        "- **\uc140\ud504 \uc5b4\ud150\uc158(Self-Attention)**: \uc2dc\ud000\uc2a4 \ub0b4 \ubaa8\ub4e0 \ud1a0\ud070 \uac04\uc758 \uad00\uacc4\ub97c \ud559\uc2b5",
        "- **\uba40\ud2f0\ud5e4\ub4dc \uc5b4\ud150\uc158(Multi-Head Attention)**: \uc5ec\ub7ec \uad00\uc810\uc5d0\uc11c \uc5b4\ud150\uc158\uc744 \uacc4\uc0b0",
        "- **\ud3ec\uc9c0\uc154\ub110 \uc778\ucf54\ub529(Positional Encoding)**: \uc21c\uc11c \uc815\ubcf4\ub97c \ubaa8\ub378\uc5d0 \uc8fc\uc785",
        "- **\ud53c\ub4dc \ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c(Feed-Forward Network)**: \ube44\uc120\ud615 \ubcc0\ud658 \uc218\ud589",
        "- **\ub808\uc774\uc5b4 \uc815\uaddc\ud654(Layer Normalization)**: \ud559\uc2b5 \uc548\uc815\ud654",
        "- **\uc794\ucc28 \uc5f0\uacb0(Residual Connection)**: \uadf8\ub798\ub514\uc5b8\ud2b8 \uc18c\uc2e4 \ubc29\uc9c0",
        "",
        "## \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc778\ucf54\ub354\ub97c \ud65c\uc6a9\ud55c \uae0d\uc815 \ubd80\uc815 \uc608\uce21",
        "",
        "\ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc778\ucf54\ub354\ub294 \uc785\ub825 \uc2dc\ud000\uc2a4\ub97c \uc778\ucf54\ub529\ud558\uc5ec \ubb38\ub9e5\uc744 \uc774\ud574\ud558\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 \uac04\ub2e8\ud55c \uac10\uc131 \ubd84\uc11d(\uae0d\uc815/\ubd80\uc815 \ubd84\ub958)\uc744 \ud1b5\ud574 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc778\ucf54\ub354\uc758 \uc6d0\ub9ac\ub97c \ud559\uc2b5\ud569\ub2c8\ub2e4.",
        "",
        "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning_4th/blob/master/colab/ch24-colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac \uc784\ud3ec\ud2b8",
        "import tensorflow as tf",
        "from tensorflow.keras.models import Model",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input",
        "from tensorflow.keras.optimizers import Adam",
        "import pandas as pd",
        "import numpy as np",
        "from sklearn.model_selection import train_test_split",
        "",
        "# \uae43\ud5c8\ube0c\uc5d0 \uc900\ube44\ub41c \ub370\uc774\ud130\ub97c \uac00\uc838\uc635\ub2c8\ub2e4.",
        "!git clone https://github.com/taehojo/data.git",
        "",
        "# CSV \ud30c\uc77c \ub85c\ub4dc",
        "dataframe = pd.read_csv('./data/sentiment_data.csv')",
        "",
        "# \ub370\uc774\ud130\uc640 \ub77c\ubca8 \ucd94\ucd9c",
        "sentences = dataframe['sentence'].tolist()",
        "labels = dataframe['label'].tolist()",
        "",
        "# \uc784\ubca0\ub529 \ubca1\ud130 \ud06c\uae30\uc640 \ucd5c\ub300 \ubb38\uc7a5 \uae38\uc774 \uc124\uc815",
        "embedding_dim = 128  # \ub2e8\uc5b4 \uc784\ubca0\ub529\uc758 \ucc28\uc6d0",
        "max_len = 10  # \ucc98\ub9ac\ud560 \ucd5c\ub300 \ub2e8\uc5b4 \uc218"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uacfc\uc815"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \ud1a0\ud06c\ub098\uc774\uc800 \ucd08\uae30\ud654 \ubc0f \ud14d\uc2a4\ud2b8\ub97c \uc2dc\ud000\uc2a4\ub85c \ubcc0\ud658",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()",
        "tokenizer.fit_on_texts(sentences)  # \ubb38\uc7a5\uc744 \ub2e8\uc5b4 \uc778\ub371\uc2a4\ub85c \ubcc0\ud658\ud558\uae30 \uc704\ud55c \uc0ac\uc804 \uad6c\ucd95",
        "sequences = tokenizer.texts_to_sequences(sentences)  # \ubb38\uc7a5\uc744 \uc22b\uc790 \uc2dc\ud000\uc2a4\ub85c \ubcc0\ud658",
        "word_index = tokenizer.word_index  # \ub2e8\uc5b4-\uc778\ub371\uc2a4 \ub9e4\ud551 \uc0ac\uc804",
        "",
        "# \ud328\ub529\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2dc\ud000\uc2a4 \uae38\uc774\ub97c \ub3d9\uc77c\ud558\uac8c \ub9de\ucda4",
        "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')",
        "",
        "# \ub370\uc774\ud130\uc14b\uc744 \ud6c8\ub828 \uc138\ud2b8\uc640 \uac80\uc99d \uc138\ud2b8\ub85c \ubd84\ub9ac",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud3ec\uc9c0\uc154\ub110 \uc778\ucf54\ub529(Positional Encoding)",
        "",
        "\ud2b8\ub79c\uc2a4\ud3ec\uba38\ub294 \uc21c\ud658 \uad6c\uc870\uac00 \uc5c6\uc5b4 \uc2dc\ud000\uc2a4\uc758 \uc21c\uc11c \uc815\ubcf4\ub97c \ubcc4\ub3c4\ub85c \uc81c\uacf5\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574 \ud3ec\uc9c0\uc154\ub110 \uc778\ucf54\ub529\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \ud3ec\uc9c0\uc154\ub110 \uc778\ucf54\ub529 \ud568\uc218",
        "def get_positional_encoding(max_len, d_model):",
        "    \"\"\"",
        "    max_len: \uc2dc\ud000\uc2a4 \ucd5c\ub300 \uae38\uc774",
        "    d_model: \uc784\ubca0\ub529 \ucc28\uc6d0",
        "    ",
        "    \uc218\uc2dd: PE(pos, 2i) = sin(pos/10000^(2i/d_model))",
        "         PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))",
        "    \"\"\"",
        "    pos_enc = np.zeros((max_len, d_model))",
        "    for pos in range(max_len):",
        "        for i in range(0, d_model, 2):",
        "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))",
        "            if i + 1 < d_model:",
        "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))",
        "    return pos_enc",
        "",
        "# \ud3ec\uc9c0\uc154\ub110 \uc778\ucf54\ub529 \uc0dd\uc131",
        "positional_encoding = get_positional_encoding(max_len, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \uba40\ud2f0\ud5e4\ub4dc \uc140\ud504 \uc5b4\ud150\uc158 \ub808\uc774\uc5b4(Multi-Head Self-Attention Layer)",
        "",
        "\uc140\ud504 \uc5b4\ud150\uc158\uc740 \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \ud575\uc2ec \uba54\ucee4\ub2c8\uc998\uc73c\ub85c, \uc2dc\ud000\uc2a4 \ub0b4 \uac01 \uc704\uce58\uac00 \ub2e4\ub978 \ubaa8\ub4e0 \uc704\uce58\uc640 \uc5bc\ub9c8\ub098 \uad00\ub828\uc788\ub294\uc9c0 \uacc4\uc0b0\ud569\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \uba40\ud2f0\ud5e4\ub4dc \uc5b4\ud150\uc158 \ub808\uc774\uc5b4 (\ub9c8\uc2a4\ud06c \ubbf8\uc0ac\uc6a9)",
        "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):",
        "    def __init__(self, num_heads, key_dim):",
        "        \"\"\"",
        "        num_heads: \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218 (\uc11c\ub85c \ub2e4\ub978 \uad00\uc810\uc5d0\uc11c \uc5b4\ud150\uc158 \uacc4\uc0b0)",
        "        key_dim: \ud0a4 \ubca1\ud130\uc758 \ucc28\uc6d0",
        "        \"\"\"",
        "        super(MultiHeadSelfAttentionLayer, self).__init__()",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)",
        "        self.norm = LayerNormalization()  # \ub808\uc774\uc5b4 \uc815\uaddc\ud654",
        "",
        "    def call(self, x):",
        "        # \uc140\ud504 \uc5b4\ud150\uc158 \uacc4\uc0b0 (\ucffc\ub9ac, \ud0a4, \uac12\uc774 \ubaa8\ub450 \ub3d9\uc77c\ud55c \uc785\ub825)",
        "        attn_output = self.mha(query=x, value=x, key=x)",
        "        # \uc794\ucc28 \uc5f0\uacb0(residual connection)\uacfc \ub808\uc774\uc5b4 \uc815\uaddc\ud654",
        "        attn_output = self.norm(attn_output + x)",
        "        return attn_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ubaa8\ub378 \uad6c\uc131: \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc778\ucf54\ub354"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \ubaa8\ub378 \uc124\uc815",
        "inputs = Input(shape=(max_len,))",
        "",
        "# 1. \uc784\ubca0\ub529 \ub808\uc774\uc5b4 - \ub2e8\uc5b4 \uc778\ub371\uc2a4\ub97c \ubca1\ud130\ub85c \ubcc0\ud658",
        "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim)",
        "embedded_sequences = embedding_layer(inputs)",
        "",
        "# 2. \ud3ec\uc9c0\uc154\ub110 \uc778\ucf54\ub529 \ucd94\uac00 - \uc704\uce58 \uc815\ubcf4 \uc8fc\uc785",
        "embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding",
        "",
        "# 3. \uccab \ubc88\uc9f8 \uba40\ud2f0\ud5e4\ub4dc \uc5b4\ud150\uc158 (\ub9c8\uc2a4\ud06c \uc5c6\uc74c)",
        "attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)",
        "attention_output = attention_layer(embedded_sequences_with_positional_encoding)",
        "",
        "# 4. \uc794\ucc28 \uc5f0\uacb0 - \uadf8\ub798\ub514\uc5b8\ud2b8 \uc18c\uc2e4 \ubc29\uc9c0",
        "attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])",
        "",
        "# 5. GlobalAveragePooling1D - \uc2dc\ud000\uc2a4\ub97c \ud558\ub098\uc758 \ubca1\ud130\ub85c \ubcc0\ud658",
        "pooled_output = GlobalAveragePooling1D()(attention_output_with_residual)",
        "",
        "# 6. \ud53c\ub4dc \ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c - \ucd5c\uc885 \ubd84\ub958",
        "dense_layer = Dense(128, activation='relu')(pooled_output)",
        "dropout_layer = Dropout(0.5)(dense_layer)  # \uacfc\uc801\ud569 \ubc29\uc9c0",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)  # \uc774\uc9c4 \ubd84\ub958(\uae0d\uc815/\ubd80\uc815)",
        "",
        "model = Model(inputs=inputs, outputs=output_layer)",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ubaa8\ub378 \ud559\uc2b5 \ubc0f \uc608\uce21"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \ubaa8\ub378 \ud559\uc2b5",
        "model.fit(X_train, np.array(y_train), epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)))",
        "",
        "# \uc0c8 \ubb38\uc7a5 \uc608\uce21",
        "sample_texts = [\"I absolutely love this!\", \"I can't stand this product\"]",
        "sample_sequences = tokenizer.texts_to_sequences(sample_texts)",
        "sample_data = tf.keras.preprocessing.sequence.pad_sequences(sample_sequences, maxlen=max_len, padding='post')",
        "predictions = model.predict(sample_data)",
        "",
        "for i, text in enumerate(sample_texts):",
        "    print(f\"Text: {text}\")",
        "    print(f\"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**\ucd9c\ub825 \uacb0\uacfc \ud574\uc11d:**",
        "\ud559\uc2b5\uc774 \uc9c4\ud589\ub428\uc5d0 \ub530\ub77c \ud6c8\ub828 \ubc0f \uac80\uc99d \uc815\ud655\ub3c4\uac00 \ube60\ub974\uac8c \uc99d\uac00\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. 10\ubc88\uc758 \uc5d0\ud3ec\ud06c \ud6c4\uc5d0\ub294 \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c 99.75%\uc758 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \uc0d8\ud50c \uc608\uce21 \uacb0\uacfc\ub3c4 \uc9c1\uad00\uc801\uc73c\ub85c \ub9de\uac8c \ubd84\ub958\ub418\uc5c8\uc2b5\ub2c8\ub2e4.",
        "",
        "## \uc804\uccb4 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uad6c\ud604\ud558\uae30",
        "",
        "\uc774\uc81c \ub9c8\uc2a4\ud0b9(masking)\uc744 \ud3ec\ud568\ud55c \uc804\uccb4 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc744 \uad6c\ud604\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ub9c8\uc2a4\ud0b9\uc740 \ub514\ucf54\ub354\uc5d0\uc11c \ubbf8\ub798 \uc815\ubcf4\ub97c \ubcf4\uc9c0 \ubabb\ud558\uac8c \ud558\ub294 \uba54\ucee4\ub2c8\uc998\uc785\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \uba40\ud2f0\ud5e4\ub4dc \uc5b4\ud150\uc158 \ub808\uc774\uc5b4(\ub9c8\uc2a4\ud06c \uc9c0\uc6d0)",
        "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):",
        "    def __init__(self, num_heads, key_dim, masked=False):",
        "        \"\"\"",
        "        num_heads: \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218",
        "        key_dim: \ud0a4 \ubca1\ud130\uc758 \ucc28\uc6d0",
        "        masked: \ub9c8\uc2a4\ud06c \uc0ac\uc6a9 \uc5ec\ubd80 (\ubbf8\ub798 \ud1a0\ud070\uc744 \ubcf4\uc9c0 \ubabb\ud558\uac8c \ud568)",
        "        \"\"\"",
        "        super(MultiHeadSelfAttentionLayer, self).__init__()",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)",
        "        self.norm = LayerNormalization()",
        "        self.masked = masked",
        "",
        "    def call(self, x):",
        "        if self.masked:",
        "            # \ub9c8\uc2a4\ud06c \uc0dd\uc131 (\ud558\uc0bc\uac01 \ud589\ub82c \ud615\ud0dc)",
        "            batch_size = tf.shape(x)[0]",
        "            seq_len = tf.shape(x)[1]",
        "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # \ud558\uc0bc\uac01 \ud589\ub82c",
        "            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))",
        "            mask = tf.tile(mask, [batch_size, 1, 1, 1])  # \ubc30\uce58 \ud06c\uae30\uc5d0 \ub9de\uac8c \ubcf5\uc81c",
        "            mask = (1-mask) * -1e9  # \ub9c8\uc2a4\ud0b9\ud560 \uc704\uce58\ub97c \ub9e4\uc6b0 \uc791\uc740 \uac12(-\u221e)\uc73c\ub85c \uc124\uc815",
        "            attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)",
        "        else:",
        "            attn_output = self.mha(query=x, value=x, key=x)",
        "        attn_output = self.norm(attn_output + x)  # \uc794\ucc28 \uc5f0\uacb0 \ubc0f \ub808\uc774\uc5b4 \uc815\uaddc\ud654",
        "        return attn_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \uc804\uccb4 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378 \uad6c\uc131"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \ubaa8\ub378 \uc124\uc815",
        "inputs = Input(shape=(max_len,))",
        "",
        "# 1. \uc784\ubca0\ub529 \ub808\uc774\uc5b4",
        "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim)",
        "embedded_sequences = embedding_layer(inputs)",
        "",
        "# 2. \ud3ec\uc9c0\uc154\ub110 \uc778\ucf54\ub529 \ucd94\uac00",
        "embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding",
        "",
        "# 3. \uccab \ubc88\uc9f8 \uba40\ud2f0\ud5e4\ub4dc \uc5b4\ud150\uc158 (\ub9c8\uc2a4\ud06c \uc5c6\uc74c) - \uc778\ucf54\ub354 \ubd80\ubd84",
        "attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)",
        "attention_output = attention_layer(embedded_sequences_with_positional_encoding)",
        "",
        "# 4. \uc794\ucc28 \uc5f0\uacb0",
        "attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])",
        "",
        "# 5. \ub9c8\uc2a4\ud06c\ub4dc \uba40\ud2f0\ud5e4\ub4dc \uc5b4\ud150\uc158 (\ub9c8\uc2a4\ud06c \uc788\uc74c) - \ub514\ucf54\ub354 \ubd80\ubd84",
        "masked_attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim, masked=True)",
        "masked_attention_output = masked_attention_layer(attention_output_with_residual)",
        "",
        "# 6. \uc794\ucc28 \uc5f0\uacb0",
        "masked_attention_output_with_residual = Add()([attention_output_with_residual, masked_attention_output])",
        "",
        "# 7. GlobalAveragePooling1D",
        "pooled_output = GlobalAveragePooling1D()(masked_attention_output_with_residual)",
        "",
        "# 8. \ud53c\ub4dc \ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c",
        "dense_layer = Dense(128, activation='relu')(pooled_output)",
        "dropout_layer = Dropout(0.5)(dense_layer)",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)",
        "",
        "model = Model(inputs=inputs, outputs=output_layer)",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ub9c8\uc2a4\ud0b9\uc758 \uc758\ubbf8\uc640 \uc2dc\uac01\ud654",
        "",
        "\ub9c8\uc2a4\ud0b9\uc740 \ub514\ucf54\ub354\uc5d0\uc11c \uc790\uae30\ud68c\uadc0\uc801(autoregressive) \ud2b9\uc131\uc744 \uad6c\ud604\ud558\uae30 \uc704\ud574 \uc911\uc694\ud569\ub2c8\ub2e4. \uac01 \uc704\uce58\ub294 \uc790\uc2e0\ubcf4\ub2e4 \uc55e\uc120 \uc704\uce58\uc758 \uc815\ubcf4\ub9cc \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \ub9c8\uc2a4\ud06c \ud589\ub82c \uc2dc\uac01\ud654 \uc608\uc81c",
        "seq_len_example = 4",
        "batch_size_example = 2",
        "mask_example = tf.linalg.band_part(tf.ones((seq_len_example, seq_len_example)), -1, 0)",
        "mask_example = tf.reshape(mask_example, (1, 1, seq_len_example, seq_len_example))",
        "mask_example = tf.tile(mask_example, [batch_size_example, 1, 1, 1])",
        "",
        "print(\"\\n\uc608\uc2dc \ub9c8\uc2a4\ud06c \ud589\ub82c :\")",
        "print(mask_example.numpy()[0, 0, :, :])",
        "# \ud558\uc0bc\uac01 \ud589\ub82c \ud615\ud0dc:",
        "# 1 0 0 0",
        "# 1 1 0 0",
        "# 1 1 1 0",
        "# 1 1 1 1",
        "",
        "mask_example = (1-mask_example) * -1e9",
        "print(\"\\n-\u221e\ub85c \uce58\ud658\ud55c \ub9c8\uc2a4\ud06c \ud589\ub82c:\")",
        "print(mask_example.numpy()[0, 0, :, :])",
        "# 0 -1e9 -1e9 -1e9",
        "# 0    0 -1e9 -1e9",
        "# 0    0    0 -1e9",
        "# 0    0    0    0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc758 \uc8fc\uc694 \ud2b9\uc9d5 \uc694\uc57d",
        "",
        "1. **\ubcd1\ub82c \ucc98\ub9ac**: RNN\uacfc \ub2ec\ub9ac \ubaa8\ub4e0 \uc704\uce58\ub97c \ubcd1\ub82c\ub85c \ucc98\ub9ac\ud558\uc5ec \ud559\uc2b5 \uc18d\ub3c4\uac00 \ube60\ub984",
        "2. **\uc7a5\uac70\ub9ac \uc758\uc874\uc131 \ud3ec\ucc29**: \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc73c\ub85c \ubb38\uc7a5 \ub0b4 \uba40\ub9ac \ub5a8\uc5b4\uc9c4 \ub2e8\uc5b4 \uac04\uc758 \uad00\uacc4\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \ud559\uc2b5",
        "3. **\ud655\uc7a5\uc131**: \uacc4\uce35\uc744 \ucd94\uac00\ud558\uc5ec \ub354 \ubcf5\uc7a1\ud55c \ud328\ud134 \ud559\uc2b5 \uac00\ub2a5",
        "4. **\uc801\uc751\uc131**: \ub2e4\uc591\ud55c NLP \ud0dc\uc2a4\ud06c\uc5d0 \uc801\uc6a9 \uac00\ub2a5",
        "",
        "## \uc2e4\uc2b5 \uacfc\uc81c",
        "",
        "1. \ucd5c\ub300 \ubb38\uc7a5 \uae38\uc774(max_len)\ub97c \ub2e4\uc591\ud558\uac8c \ubcc0\uacbd\ud558\uba70 \uc131\ub2a5 \ubcc0\ud654 \uad00\ucc30\ud558\uae30",
        "2. \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218(num_heads)\ub97c \uc870\uc815\ud558\uc5ec \uacb0\uacfc \ube44\uad50\ud558\uae30",
        "3. \ub2e4\ub978 \uac10\uc131 \ubd84\uc11d \ub370\uc774\ud130\uc14b\uc5d0 \uc801\uc6a9\ud574\ubcf4\uae30",
        "4. \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ub808\uc774\uc5b4\ub97c \ub354 \ucd94\uac00\ud558\uc5ec \ubaa8\ub378\uc758 \uae4a\uc774\uac00 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5 \ud655\uc778\ud558\uae30",
        "",
        "## \ucc38\uace0 \uc790\ub8cc",
        "",
        "- \"Attention Is All You Need\" \ub17c\ubb38 (Vaswani et al., 2017)",
        "- [\ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378 \uc2dc\uac01\ud654](http://jalammar.github.io/illustrated-transformer/)",
        "- [\ud150\uc11c\ud50c\ub85c\uc6b0 \uacf5\uc2dd \ud29c\ud1a0\ub9ac\uc5bc: \ud2b8\ub79c\uc2a4\ud3ec\uba38](https://www.tensorflow.org/tutorials/text/transformer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}